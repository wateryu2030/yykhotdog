"""
ETLæ­¥éª¤08: é”€é‡/è¥æ”¶é¢„æµ‹
åŸºäºOpenAIå»ºè®®çš„ä¼˜åŒ–ç‰ˆæœ¬
"""
import sys
import os
import pandas as pd
import numpy as np
import datetime as dt
from pathlib import Path

# æ·»åŠ libè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from lib.mssql import fetch_df, to_sql, get_conn, get_table_count, execute_sql
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DW = "hotdog2030"
HORIZON = 7  # é¢„æµ‹æœªæ¥7å¤©

def load_store_daily():
    """åŠ è½½é—¨åº—æ¯æ—¥é”€å”®æ•°æ®"""
    logger.info("ğŸ“Š å¼€å§‹åŠ è½½é—¨åº—æ¯æ—¥é”€å”®æ•°æ®...")
    
    sql = """
    SELECT date_key, store_id, revenue
    FROM dbo.vw_sales_store_daily
    """
    
    df = fetch_df(sql, DW)
    logger.info(f"âœ… é—¨åº—æ¯æ—¥é”€å”®æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡è®°å½•")
    return df

def make_features(df):
    """åˆ›å»ºç‰¹å¾å·¥ç¨‹"""
    logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
    
    df = df.copy()
    df['date'] = pd.to_datetime(df['date_key'].astype(str))
    df = df.sort_values(['store_id','date'])
    
    # ç§»åŠ¨å¹³å‡ç‰¹å¾
    for w in [7,14,28]:
        df[f'rev_ma_{w}'] = df.groupby('store_id')['revenue'].transform(lambda s: s.rolling(w, min_periods=1).mean())
    
    # æ—¶é—´ç‰¹å¾
    df['dow'] = df['date'].dt.weekday
    
    df = df.dropna()
    logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(df)} æ¡è®°å½•")
    return df

def forecast_per_store(g):
    """ä¸ºå•ä¸ªé—¨åº—ç”Ÿæˆé¢„æµ‹"""
    try:
        # å°è¯•ä½¿ç”¨XGBoost
        from xgboost import XGBRegressor
        MODEL = "xgboost"
    except Exception:
        # å›é€€åˆ°sklearn
        from sklearn.ensemble import HistGradientBoostingRegressor
        def XGBRegressor(**kw): return HistGradientBoostingRegressor()
        MODEL = "hgb"
    
    X = g[['rev_ma_7','rev_ma_14','rev_ma_28','dow']]
    y = g['revenue']
    
    model = XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05)
    model.fit(X, y)
    
    # é€æ—¥æ»šåŠ¨é¢„æµ‹ç®€å•ç‰ˆï¼šç”¨æœ€åä¸€æ—¥ç‰¹å¾è¿‘ä¼¼
    last = g.iloc[-1:].copy()
    preds = []
    base_date = pd.to_datetime(str(int(last['date_key'].iloc[0])), format='%Y%m%d')
    ma7, ma14, ma28 = last[['rev_ma_7','rev_ma_14','rev_ma_28']].values[0]
    
    for i in range(1, HORIZON+1):
        d = base_date + dt.timedelta(days=i)
        dow = d.weekday()
        Xf = [[ma7, ma14, ma28, dow]]
        yhat = float(model.predict(Xf)[0])
        preds.append((int(d.strftime('%Y%m%d')), int(g['store_id'].iloc[0]), yhat))
        
        # ç®€å•æ»šåŠ¨ï¼šæŠŠé¢„æµ‹çº³å…¥ ma7/14/28
        ma7 = (ma7*6 + yhat)/7
        ma14 = (ma14*13 + yhat)/14
        ma28 = (ma28*27 + yhat)/28
    
    return preds

def write_preds(rows):
    """å†™å…¥é¢„æµ‹ç»“æœ"""
    if not rows: 
        logger.warning("âš ï¸ æ²¡æœ‰é¢„æµ‹ç»“æœå¯å†™å…¥")
        return
    
    logger.info("ğŸ’¾ å¼€å§‹å†™å…¥é¢„æµ‹ç»“æœ...")
    
    try:
        from xgboost import XGBRegressor
        MODEL = "xgboost"
    except Exception:
        MODEL = "hgb"
    
    with get_conn(DW) as conn:
        cur = conn.cursor()
        success_count = 0
        
        for date_key, store_id, yhat in rows:
            try:
                cur.execute("""
MERGE dbo.fact_forecast_daily AS T
USING (SELECT ? AS date_key, ? AS store_id) AS S
ON (T.date_key=S.date_key AND T.store_id=S.store_id)
WHEN MATCHED THEN UPDATE SET yhat=?, model_name=?, created_at=sysutcdatetime()
WHEN NOT MATCHED THEN INSERT (date_key, store_id, yhat, model_name) VALUES (?, ?, ?, ?);
""", 
                    date_key, store_id, yhat, MODEL, date_key, store_id, yhat, MODEL
                )
                success_count += 1
            except Exception as e:
                logger.warning(f"âš ï¸ è·³è¿‡é¢„æµ‹å†™å…¥: {str(e)}")
                continue
        
        conn.commit()
        logger.info(f"âœ… é¢„æµ‹ç»“æœå†™å…¥å®Œæˆ: {success_count} æ¡è®°å½•")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ETLæ­¥éª¤08: é”€é‡/è¥æ”¶é¢„æµ‹")
    
    try:
        # åŠ è½½é—¨åº—æ¯æ—¥é”€å”®æ•°æ®
        df = load_store_daily()
        
        if df.empty:
            logger.warning("âš ï¸ æ²¡æœ‰é”€å”®æ•°æ®å¯é¢„æµ‹")
            return
        
        # ç‰¹å¾å·¥ç¨‹
        df = make_features(df)
        
        if df.empty:
            logger.warning("âš ï¸ ç‰¹å¾å·¥ç¨‹åæ²¡æœ‰æ•°æ®å¯é¢„æµ‹")
            return
        
        # ä¸ºæ¯ä¸ªé—¨åº—ç”Ÿæˆé¢„æµ‹
        all_preds = []
        for _, g in df.groupby('store_id'):
            if len(g) < 30:  # æ•°æ®å¤ªå°‘è·³è¿‡
                continue
            all_preds += forecast_per_store(g)
        
        # å†™å…¥é¢„æµ‹ç»“æœ
        write_preds(all_preds)
        
        # éªŒè¯ç»“æœ
        count = get_table_count(DW, "fact_forecast_daily")
        logger.info(f"ğŸ‰ ETLæ­¥éª¤08å®Œæˆ! fact_forecast_dailyè¡¨ç°åœ¨æœ‰ {count} æ¡è®°å½•")
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ“Š é”€å”®é¢„æµ‹ç»Ÿè®¡:")
        logger.info(f"   - é¢„æµ‹è®°å½•æ•°: {len(all_preds)}")
        if all_preds:
            avg_pred = np.mean([pred[2] for pred in all_preds])
            logger.info(f"   - å¹³å‡é¢„æµ‹é‡‘é¢: {avg_pred:.2f}")
            logger.info(f"   - é¢„æµ‹é—¨åº—æ•°: {len(set([pred[1] for pred in all_preds]))}")
            
    except Exception as e:
        logger.error(f"âŒ ETLæ­¥éª¤08æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    main()